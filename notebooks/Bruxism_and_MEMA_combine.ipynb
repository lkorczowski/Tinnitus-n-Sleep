{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Trial to combine middle ear and bruxism tagging to identify pure moments of middle ear activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PATH = os.getcwd() \n",
    "import sys\n",
    "sys.path.append(PATH + '/../')\n",
    "import mne\n",
    "from tinnsleep.config import Config\n",
    "from tinnsleep.create_reports import preprocess, reporting\n",
    "from tinnsleep.data import CreateRaw, RawToEpochs_sliding, CleanAnnotations, AnnotateRaw_sliding\n",
    "from tinnsleep.classification import AmplitudeThresholding\n",
    "from tinnsleep.check_impedance import create_annotation_mne, Impedance_thresholding_sliding, check_RMS, fuse_with_classif_result\n",
    "from tinnsleep.signal import rms\n",
    "from tinnsleep.scoring import classif_to_burst, burst_to_episode, create_list_events\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy\n",
    "from tinnsleep.config import Config\n",
    "\n",
    "print(\"Config loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E:/Acou_sommeil/EDF_V2_PAUL\\\\robin_nuit_23_sept.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\robin_nuit_son_24_sept.edf']\n",
      "\n",
      "['E:/Acou_sommeil/EDF_V2_PAUL\\\\1DA15_nuit_son.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\1GB19_nuit_hab.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\1MF19_nuit_hab.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\1RA17_nuit_hab.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\HZB_nuit_1.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\HZB_nuit_2.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\HZB_nuit_3.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\SCHMIDTLIN_nuit_1_dec_OD__0to0.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\SCHMIDTLIN_nuit_3_dec_OD__4to0to2.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\SCHMIDTLIN_nuit_4_dec_OD__3to3.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\SCHMIDTLIN_nuit_5_dec_OD__0to1.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\SCHM_nuit_1.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\SCHM_nuit_2.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\SCHM_nuit_3.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\Schmidtlin_nuit_2_dec_3to0to4.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\Unger_2.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\jon_mema.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\jose_mema.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\robin_mema_nuit_1.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\robin_mema_nuit_2.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\sophie_mema.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\tom_mema.edf', 'E:/Acou_sommeil/EDF_V2_PAUL\\\\unger_nuit_1.edf']\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "#List of MEMA files, hardcoded, to be modified\n",
    "print(Config.bruxisme_files[44:46])\n",
    "print(\"\")\n",
    "ME_files=[Config.bruxisme_files[5], Config.bruxisme_files[9], \n",
    "          Config.bruxisme_files[14], Config.bruxisme_files[22]]\n",
    "ME_files.extend(Config.bruxisme_files[28:44])\n",
    "ME_files.extend(Config.bruxisme_files[46:])\n",
    "\n",
    "print(ME_files)\n",
    "print(len(ME_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting down parameters for bruxism detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters set\n"
     ]
    }
   ],
   "source": [
    "#Setting parameters\n",
    "os.chdir(\"C:/Users/Zeta/Documents/acou_sommeil_HD_ENS/Tinnitus-n-Sleep/Notebooks\")\n",
    "THR_classif=[[0,2]]\n",
    "sfreq = 250\n",
    "window_length = 0.25                    # in seconds\n",
    "duration = int(window_length * sfreq)   # in samples\n",
    "interval = duration                     # no overlapping\n",
    "n_adaptive = 480 # number of epochs for adaptative baseline\n",
    "\n",
    "#Importing personnalized parameters for dataset\n",
    "df = pd.read_pickle(\"data/valid_chans_THR_imp.pk\")\n",
    "dico_chans= df.to_dict(\"list\")\n",
    "\n",
    "delim =3\n",
    "print(\"parameters set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bruxism + MEMA processing for pure MEMA visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed : \n",
      "jon_mema.edf\n",
      "['1', '2']\n",
      "Bruxism reporting done\n",
      "Data filtered\n",
      "Epochs done, shape (33680, 1, 250)\n",
      "Total number of middle ear episodes\n",
      "239\n",
      "Number of pure episodes\n",
      "197\n",
      "Number of combined episodes\n",
      "35\n",
      "Number of episodes rejected due to artifacts:\n",
      "7\n",
      "Raw annotated\n"
     ]
    }
   ],
   "source": [
    "filenames = ME_files[16:17]  # load file from config\n",
    "#filenames=['E:/Acou_sommeil/EDF_V2_PAUL\\\\sophie_mema.edf']\n",
    "\n",
    "#Output of the processing, stock pure MEMA analysis output\n",
    "ME_reports={}\n",
    "results={}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    print(\"Files processed : \")\n",
    "    \n",
    "    #Loop on all the patient files\n",
    "    for filename in filenames:\n",
    "        \n",
    "        #-----------------Preparation for bruxism  processing ---------------------------------------\n",
    "        #opens the raw file\n",
    "        raw = mne.io.read_raw_edf(filename, preload=False, verbose=False)  # prepare loading\n",
    "        print(filename.split(\"\\\\\")[-1])\n",
    "        #Get channels indexes\n",
    "        ind_picks_chan= dico_chans[filename.split(\"\\\\\")[-1]][0]\n",
    "        ind_picks_imp= dico_chans[filename.split(\"\\\\\")[-1]][1]\n",
    "        #Get THR_imp value for filename\n",
    "        THR_imp = dico_chans[filename.split(\"\\\\\")[-1]][2]\n",
    "        #Get channel names from indexes\n",
    "        if len(ind_picks_chan)>0: #ignore file if no channel is good\n",
    "            picks_chan=[]\n",
    "            for elm in ind_picks_chan:\n",
    "                picks_chan.append(raw.info[\"ch_names\"][elm])\n",
    "            picks_imp=[]\n",
    "            for elm in ind_picks_imp:\n",
    "                picks_imp.append(raw.info[\"ch_names\"][elm])\n",
    "            print(picks_chan)\n",
    "            #Setting parameters for is_good\n",
    "            params = dict(ch_names=picks_chan,\n",
    "                  rejection_thresholds=dict(emg=5e-04),  # two order of magnitude higher q0.01\n",
    "                  flat_thresholds=dict(emg=1e-09),  # one order of magnitude lower median\n",
    "                  channel_type_idx=dict(emg=[ i for i in range(len(picks_chan))]),\n",
    "                  full_report=True\n",
    "                  )\n",
    "            #Epoching parameters\n",
    "            window_length = 0.25                    # in seconds\n",
    "            duration = int(window_length * sfreq)   # in samples\n",
    "            interval = duration                     # no overlapping\n",
    "            \n",
    "            \n",
    "            #-----------------Bruxism  processing ---------------------------------------\n",
    "            # Get the preprocessing steps done\n",
    "            epochs, valid_labels, log = preprocess(raw, picks_chan, picks_imp, duration, interval, \n",
    "                                                   params, THR_imp=THR_imp, get_log=True)\n",
    "            if np.sum(valid_labels)>0 : #If at least one epoch is good create report\n",
    "                results[filename] = reporting(epochs, valid_labels, THR_classif, time_interval=window_length, delim=delim, n_adaptive = n_adaptive, log=log)\n",
    "                #List of labels for MEMA processing crossing\n",
    "                bruxism = results[filename][\"labels\"][0]\n",
    "                artefacts = np.invert(valid_labels)\n",
    "                \n",
    "                #------------grouping bruxism episodes and bursts together------------------------------------\n",
    "                burst_list = classif_to_burst(bruxism, time_interval=0.25)\n",
    "                li_ep = burst_to_episode(burst_list, delim=3)\n",
    "                event_list = create_list_events(li_ep, 0.25, len(bruxism) * 0.25)\n",
    "                \n",
    "                #All episodes as tonic\n",
    "                for i in range(len(event_list)):\n",
    "                    if event_list[i]!=0:\n",
    "                        event_list[i]=True\n",
    "                    else:\n",
    "                        event_list[i] = False\n",
    "\n",
    "                bruxism_c = np.any(np.c_[bruxism, event_list], axis=-1) #rassemblement des bursts et des episodes de bruxismes\n",
    "\n",
    "            \n",
    "                print(\"Bruxism reporting done\")\n",
    "\n",
    "                #-----------------MEMA processing preparation ---------------------------------------\n",
    "                picks_chan = ['Airflow']           # middle ear electrode\n",
    "                raw  = CreateRaw(raw[picks_chan][0], raw.info[\"sfreq\"], picks_chan, ch_types=['emg']) # pick channels and load\n",
    "                ch_names = raw.info[\"ch_names\"]\n",
    "                print(\"Data filtered\")\n",
    "\n",
    "                #epoching\n",
    "                sfreq = raw.info[\"sfreq\"]\n",
    "                window_length = 1                    # in seconds\n",
    "                duration = int(window_length * sfreq)   # in samples\n",
    "                interval = duration                     # no overlapping\n",
    "                epochs = RawToEpochs_sliding(raw, duration=duration, interval=interval)\n",
    "                print(f\"Epochs done, shape {epochs.shape}\")\n",
    "\n",
    "                #-----------------MEMA processing Foward-backward ---------------------------------------\n",
    "                #Foward\n",
    "                # compute the sum of power over electrodes and samples in each window\n",
    "                pipeline = AmplitudeThresholding(abs_threshold=0., rel_threshold=4, n_adaptive=60)\n",
    "                X        = rms(epochs) # take only valid labels\n",
    "                labels_f   = pipeline.fit_predict(X)\n",
    "\n",
    "\n",
    "                #Backward\n",
    "                #Reversing epochs array\n",
    "                epochs = epochs[::-1]\n",
    "                 # compute the sum of power over electrodes and samples in each window\n",
    "                pipeline = AmplitudeThresholding(abs_threshold=0., rel_threshold=4, n_adaptive=60)\n",
    "                X        = rms(epochs) # take only valid labels\n",
    "                labels   = pipeline.fit_predict(X)\n",
    "                #Reversing labels\n",
    "                labels_b = labels[::-1]\n",
    "                \n",
    "                \n",
    "                #-----------------MEMA foward-backward merge and epochs length adaptation---------------------------------------\n",
    "                # Logical OR -- merged backward and foward\n",
    "                labels_fb = np.any(np.c_[labels_f, labels_b], axis=-1)\n",
    "                \n",
    "                #adaptation of labels_fb from 1s epochs to 0,25s epochs\n",
    "                labels_fb_shep=[]  \n",
    "                for elm in labels_fb:\n",
    "                    for i in range(4):\n",
    "                        labels_fb_shep.append(elm)\n",
    "                 \n",
    "                #-----------------Pure MEMA bursts conversion to episodes ----------------------------------------\n",
    "                OM_burst = classif_to_burst(labels_fb_shep, time_interval=0.25)\n",
    "                OM_ep= burst_to_episode(OM_burst, delim=2, min_burst_joining=0)\n",
    "                li_OM = create_list_events(OM_ep, 0.25, 0.25* len(bruxism))\n",
    "                \n",
    "                burst_list = classif_to_burst(bruxism, time_interval=0.25)\n",
    "                \n",
    "                \n",
    "                #---------------Classifying MEMA events according to bruxism----------------------\n",
    "                comb_ep=[]\n",
    "                pure_ep=[]\n",
    "                compt_arti=0\n",
    "                #merge MEMA and compare with bruxism and artefacts\n",
    "                for elm in OM_ep:\n",
    "                    if np.sum(artefacts[int(elm.beg/0.25):int(elm.end/0.25)]) == 0:\n",
    "                        if np.sum(bruxism_c[int(elm.beg/0.25):int(elm.end/0.25)])>0 :\n",
    "                            comb_ep.append(elm)\n",
    "                        else:\n",
    "                            pure_ep.append(elm)\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        compt_arti+=1\n",
    "                \n",
    "                #------------------\n",
    "                #Pure episodes creation\n",
    "                li_OM_p = create_list_events(pure_ep, 0.25, 0.25* len(bruxism))\n",
    "                #Combined episode creation\n",
    "                li_OM_c = create_list_events(comb_ep, 0.25, 0.25* len(bruxism))\n",
    "\n",
    "\n",
    "\n",
    "                #All episodes as tonic\n",
    "                for i in range(len(li_OM_p)):\n",
    "                    if li_OM_p[i]!=0:\n",
    "                        li_OM_p[i]=2\n",
    "                    else:\n",
    "                        li_OM_p[i] = False\n",
    "\n",
    "                #All episodes as tonic\n",
    "                for i in range(len(li_OM_c)):\n",
    "                    if li_OM_c[i]!=0:\n",
    "                        li_OM_c[i]=3\n",
    "                    else:\n",
    "                        li_OM_c[i] = False\n",
    "\n",
    "\n",
    "\n",
    "                print(\"Total number of middle ear episodes\")\n",
    "                print(len(OM_ep))\n",
    "                print(\"Number of pure episodes\")\n",
    "                print(len(pure_ep))\n",
    "                print(\"Number of combined episodes\")\n",
    "                print(len(comb_ep))\n",
    "                print(\"Number of episodes rejected due to artifacts:\")\n",
    "                print(compt_arti)\n",
    "\n",
    "\n",
    "\n",
    "                #-----------------Pure MEMA episodes visualisation and comparison with ATM activity -----------------------------------\n",
    "                #Preparing raw for visualisation\n",
    "                picks_chan = ['Airflow', '1', '2']           # subset of EMG electrodes\n",
    "                raw  = mne.io.read_raw_edf(filename, preload=False, verbose=False)  # prepare loading\n",
    "                raw  = CreateRaw(raw[picks_chan][0], raw.info[\"sfreq\"], picks_chan, ch_types='emg')        # pick channels and load\n",
    "                raw  = raw.load_data()\n",
    "                dat=raw.get_data()\n",
    "                dat[1]=[dat[1][i]*1/(0.0005) for i in range(len(dat[1]))]\n",
    "                dat[2]=[dat[2][i]*1/(0.0005) for i in range(len(dat[2]))]\n",
    "                raw  = CreateRaw(dat, raw.info[\"sfreq\"], picks_chan, ch_types='emg') \n",
    "\n",
    "                raw  = raw.filter(20., 99., n_jobs=4, \n",
    "                                  fir_design='firwin', filter_length='auto', phase='zero-double',\n",
    "                                  picks=['1', '2'])\n",
    "\n",
    "                #Annotating the raw\n",
    "                raw = CleanAnnotations(raw)\n",
    "                dict_annotations = {1: \"bruxism\", 2:\"pure\", 3:\"comb\", 4:\"tot\"}\n",
    "                raw = AnnotateRaw_sliding(raw, li_OM_p, \n",
    "                                dict_annotations=dict_annotations, duration=50, interval=50)\n",
    "                raw = AnnotateRaw_sliding(raw, li_OM_c, \n",
    "                                dict_annotations=dict_annotations, duration=50, interval=50)\n",
    "\n",
    "                print(\"Raw annotated\")\n",
    "                raw.plot(scalings = \"auto\")\n",
    "                plt.title(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small function to check if all channel selections are good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed : \n",
      "1BA07_nuit_hab.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1BA07_nuit_son.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1CC05_nuit_hab.edf\n",
      "['1', '2']\n",
      "['1 Impedance', '2 Impedance']\n",
      "\n",
      "1CC05_nuit_son.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1DA15_nuit_hab.edf\n",
      "['1']\n",
      "['1 Imp?dance']\n",
      "\n",
      "1DA15_nuit_son.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1DL12_nuit_hab.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1DL12_nuit_son.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1GB18_nuit_son.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1GB19_nuit_hab.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1GF14_nuit_hab.edf\n",
      "1GF14_nuit_son.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1MA16_nuit_hab.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1MA16_nuit_son.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1MF19_nuit_hab.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1MF19_nuit_son.edf\n",
      "['1', '2']\n",
      "['1 Impedance', '2 Impedance']\n",
      "\n",
      "1MN09_nuit_hab.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1MN09_nuit_son.edf\n",
      "['1', '2']\n",
      "['1 Impedance', '2 Impedance']\n",
      "\n",
      "1PI07_nuit_hab.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1PI07_nuit_son.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1PT06_nuit_hab.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1PT06_nuit_son.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1RA17_nuit_hab.edf\n",
      "1RA17_nuit_son.edf\n",
      "['1', '2']\n",
      "['1 Impedance', '2 Impedance']\n",
      "\n",
      "1SA14_nuit_hab.edf\n",
      "['1', '2']\n",
      "['1 Impedance', '2 Impedance']\n",
      "\n",
      "1SA14_nuit_son.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "1ZN04_nuit_hab.edf\n",
      "['1', '2']\n",
      "['1 Impedance', '2 Impedance']\n",
      "\n",
      "1ZN04_nuit_son.edf\n",
      "['1', '2']\n",
      "['1 Impedance', '2 Impedance']\n",
      "\n",
      "HZB_nuit_1.edf\n",
      "['2']\n",
      "['2 Imp?dance']\n",
      "\n",
      "HZB_nuit_2.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "HZB_nuit_3.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "SCHMIDTLIN_nuit_1_dec_OD__0to0.edf\n",
      "SCHMIDTLIN_nuit_3_dec_OD__4to0to2.edf\n",
      "['1']\n",
      "['1 Imp?dance']\n",
      "\n",
      "SCHMIDTLIN_nuit_4_dec_OD__3to3.edf\n",
      "['1']\n",
      "['1 Imp?dance']\n",
      "\n",
      "SCHMIDTLIN_nuit_5_dec_OD__0to1.edf\n",
      "['1']\n",
      "['1 Imp?dance']\n",
      "\n",
      "SCHM_nuit_1.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "SCHM_nuit_2.edf\n",
      "['1', '2']\n",
      "['1 Impedance', '2 Impedance']\n",
      "\n",
      "SCHM_nuit_3.edf\n",
      "['1', '2']\n",
      "['1 Impedance', '2 Impedance']\n",
      "\n",
      "Schmidtlin_nuit_2_dec_3to0to4.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "Unger_2.edf\n",
      "['1', '2']\n",
      "['1 Impedance', '2 Impedance']\n",
      "\n",
      "jon_mema.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "jose_mema.edf\n",
      "['2']\n",
      "['2 Imp?dance']\n",
      "\n",
      "robin_mema_nuit_1.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "robin_mema_nuit_2.edf\n",
      "['1', '2']\n",
      "['1 Imp?dance', '2 Imp?dance']\n",
      "\n",
      "robin_nuit_23_sept.edf\n",
      "['1', '2']\n",
      "['1 Impedance', '2 Impedance']\n",
      "\n",
      "robin_nuit_son_24_sept.edf\n",
      "['1', '2']\n",
      "['1 Impedance', '2 Impedance']\n",
      "\n",
      "sophie_mema.edf\n",
      "['2']\n",
      "['2 Imp?dance']\n",
      "\n",
      "tom_mema.edf\n",
      "['2']\n",
      "['2 Imp?dance']\n",
      "\n",
      "unger_nuit_1.edf\n",
      "['1', '2']\n",
      "['1 Impedance', '2 Impedance']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "labels_subj={}\n",
    "EDF_list = Config.bruxisme_files\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    results={}\n",
    "    print(\"Files processed : \")\n",
    "    for filename in EDF_list:\n",
    "        #opens the raw file\n",
    "        raw = mne.io.read_raw_edf(filename, preload=False, verbose=False)  # prepare loading\n",
    "        print(filename.split(\"\\\\\")[-1])\n",
    "        #Get channels indexes\n",
    "        ind_picks_chan= dico_chans[filename.split(\"\\\\\")[-1]][0]\n",
    "        ind_picks_imp= dico_chans[filename.split(\"\\\\\")[-1]][1]\n",
    "        #Get THR_imp value for filename\n",
    "        THR_imp = dico_chans[filename.split(\"\\\\\")[-1]][2]\n",
    "        #print(raw.info[\"ch_names\"])\n",
    "        #Get channel names from indexes\n",
    "        if len(ind_picks_chan)>0: #ignore file if no channel is good\n",
    "            picks_chan=[]\n",
    "            for elm in ind_picks_chan:\n",
    "                picks_chan.append(raw.info[\"ch_names\"][elm])\n",
    "            picks_imp=[]\n",
    "            for elm in ind_picks_imp:\n",
    "                picks_imp.append(raw.info[\"ch_names\"][elm])\n",
    "            print(picks_chan)\n",
    "            print(picks_imp)\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
